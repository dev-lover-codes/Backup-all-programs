// Gemini AI Service - Core AI Integration
export class GeminiAI {
    constructor(apiKey) {
        this.apiKey = apiKey;
        this.endpoint = 'https://generativelanguage.googleapis.com/v1beta/models/gemini-pro:generateContent';
        this.requestCount = 0;
        this.lastRequestTime = Date.now();
    }

    async generateContent(prompt, options = {}) {
        // Rate limiting check
        if (!this.checkRateLimit()) {
            throw new Error('Rate limit exceeded. Please wait before making more requests.');
        }

        const requestBody = {
            contents: [{
                parts: [{
                    text: prompt
                }]
            }],
            generationConfig: {
                temperature: options.temperature || 0.7,
                topK: options.topK || 40,
                topP: options.topP || 0.95,
                maxOutputTokens: options.maxOutputTokens || 2048,
            },
            safetySettings: [
                {
                    category: "HARM_CATEGORY_HARASSMENT",
                    threshold: "BLOCK_MEDIUM_AND_ABOVE"
                },
                {
                    category: "HARM_CATEGORY_HATE_SPEECH",
                    threshold: "BLOCK_MEDIUM_AND_ABOVE"
                },
                {
                    category: "HARM_CATEGORY_SEXUALLY_EXPLICIT",
                    threshold: "BLOCK_MEDIUM_AND_ABOVE"
                },
                {
                    category: "HARM_CATEGORY_DANGEROUS_CONTENT",
                    threshold: "BLOCK_MEDIUM_AND_ABOVE"
                }
            ]
        };

        try {
            const response = await fetch(`${this.endpoint}?key=${this.apiKey}`, {
                method: 'POST',
                headers: {
                    'Content-Type': 'application/json',
                },
                body: JSON.stringify(requestBody)
            });

            if (!response.ok) {
                const error = await response.json();
                throw new Error(`Gemini API error: ${error.error?.message || 'Unknown error'}`);
            }

            const data = await response.json();

            this.requestCount++;

            // Extract text from response
            const text = data.candidates?.[0]?.content?.parts?.[0]?.text;

            if (!text) {
                throw new Error('No content generated by Gemini');
            }

            return text;
        } catch (error) {
            console.error('âŒ Gemini API error:', error);
            throw error;
        }
    }

    async generateStructuredContent(prompt, schema) {
        // Generate content and parse as JSON
        const fullPrompt = `${prompt}\n\nRespond ONLY with valid JSON matching this schema: ${JSON.stringify(schema)}`;

        const response = await this.generateContent(fullPrompt);

        try {
            // Extract JSON from response (handle markdown code blocks)
            const jsonMatch = response.match(/```json\n([\s\S]*?)\n```/) ||
                response.match(/```\n([\s\S]*?)\n```/) ||
                [null, response];

            return JSON.parse(jsonMatch[1].trim());
        } catch (error) {
            console.error('Failed to parse JSON response:', response);
            throw new Error('Failed to parse structured response from Gemini');
        }
    }

    async analyzeImage(imageData, prompt) {
        // For image analysis, we'd use gemini-pro-vision
        // This is a placeholder for future implementation
        throw new Error('Image analysis not yet implemented');
    }

    async chat(messages, options = {}) {
        // Multi-turn conversation
        const prompt = messages.map(msg =>
            `${msg.role === 'user' ? 'User' : 'Assistant'}: ${msg.content}`
        ).join('\n\n');

        return await this.generateContent(prompt, options);
    }

    checkRateLimit() {
        const now = Date.now();
        const timeDiff = now - this.lastRequestTime;

        // Reset counter every minute
        if (timeDiff > 60000) {
            this.requestCount = 0;
            this.lastRequestTime = now;
        }

        // Check if under limit (60 requests per minute)
        return this.requestCount < 60;
    }

    async embedText(text) {
        // Text embedding for semantic search
        // Would use embedding model endpoint
        throw new Error('Text embedding not yet implemented');
    }

    async streamContent(prompt, onChunk) {
        // Streaming responses for real-time output
        // This would require SSE or similar streaming mechanism
        throw new Error('Streaming not yet implemented');
    }
}

export default GeminiAI;
