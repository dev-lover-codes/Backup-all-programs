{"cells":[{"cell_type":"markdown","source":"This is a comprehensive guide to building your own AI assistant and integrating it into VS Code. We will progress from a simple script to a fully integrated VS Code extension that talks to a custom Python agent.\n\n### **Phase 1: The Simple AI Assistant (Python)**\n\nFirst, let's build the \"brain.\" We'll use Python and the OpenAI API.\n\n**Prerequisites:**\n\n  * Python installed.\n  * `pip install openai flask python-dotenv`\n\n**File:** `simple_assistant.py`","metadata":{}},{"cell_type":"code","source":"import os\nfrom openai import OpenAI\n\n# 1. Setup client (Use environment variable for security later)\nclient = OpenAI(api_key=\"YOUR_OPENAI_API_KEY\") \n\ndef ask_ai(question, code=None):\n    prompt = f\"User Question: {question}\\n\"\n    if code:\n        prompt += f\"\\nCode Context:\\n{code}\"\n\n    response = client.chat.completions.create(\n        model=\"gpt-4o\", # or gpt-3.5-turbo\n        messages=[\n            {\"role\": \"system\", \"content\": \"You are a helpful coding assistant.\"},\n            {\"role\": \"user\", \"content\": prompt}\n        ]\n    )\n    return response.choices[0].message.content\n\n# 2. Test it out\nif __name__ == \"__main__\":\n    print(ask_ai(\"Explain this code\", \"print('Hello World')\"))","outputs":[],"execution_count":null,"metadata":{}},{"cell_type":"markdown","source":"-----\n\n### **Phase 2: The Advanced Agent (Python with Tools)**\n\nNow we upgrade the assistant to an **Agent**. An agent doesn't just talk; it *acts*. We will give it tools to read files and run commands.\n\n**File:** `agent_backend/agent.py`","metadata":{}},{"cell_type":"code","source":"import os\nimport subprocess\nimport json\nfrom openai import OpenAI\n\nclient = OpenAI(api_key=os.getenv(\"OPENAI_API_KEY\"))\n\n# --- TOOLS ---\ndef read_file(file_path):\n    \"\"\"Reads a file from the codebase.\"\"\"\n    try:\n        with open(file_path, 'r') as f:\n            return f.read()\n    except Exception as e:\n        return str(e)\n\ndef list_files(directory=\".\"):\n    \"\"\"Lists files to analyze project structure.\"\"\"\n    return str(os.listdir(directory))\n\ndef run_command(command):\n    \"\"\"Executes a terminal command (Use with caution!).\"\"\"\n    try:\n        result = subprocess.run(command, shell=True, capture_output=True, text=True)\n        return result.stdout + result.stderr\n    except Exception as e:\n        return str(e)\n\n# Tool Definitions for OpenAI\ntools = [\n    {\n        \"type\": \"function\",\n        \"function\": {\n            \"name\": \"read_file\",\n            \"description\": \"Read contents of a file\",\n            \"parameters\": {\n                \"type\": \"object\",\n                \"properties\": {\"file_path\": {\"type\": \"string\"}},\n                \"required\": [\"file_path\"]\n            }\n        }\n    },\n    {\n        \"type\": \"function\",\n        \"function\": {\n            \"name\": \"run_command\",\n            \"description\": \"Run a shell command\",\n            \"parameters\": {\n                \"type\": \"object\",\n                \"properties\": {\"command\": {\"type\": \"string\"}},\n                \"required\": [\"command\"]\n            }\n        }\n    }\n]\n\ndef run_agent(user_prompt):\n    messages = [{\"role\": \"system\", \"content\": \"You are an autonomous coding agent.\"},\n                {\"role\": \"user\", \"content\": user_prompt}]\n\n    # 1. First call to LLM\n    response = client.chat.completions.create(\n        model=\"gpt-4o\",\n        messages=messages,\n        tools=tools,\n        tool_choice=\"auto\"\n    )\n    msg = response.choices[0].message\n    \n    # 2. Check if LLM wants to use a tool\n    if msg.tool_calls:\n        messages.append(msg) # Add assistant's thought to history\n        \n        for tool_call in msg.tool_calls:\n            func_name = tool_call.function.name\n            args = json.loads(tool_call.function.arguments)\n            \n            output = \"\"\n            if func_name == \"read_file\":\n                output = read_file(args[\"file_path\"])\n            elif func_name == \"run_command\":\n                output = run_command(args[\"command\"])\n            \n            # 3. Feed tool output back to LLM\n            messages.append({\n                \"tool_call_id\": tool_call.id,\n                \"role\": \"tool\",\n                \"name\": func_name,\n                \"content\": output\n            })\n\n        # 4. Final response after tool use\n        final_response = client.chat.completions.create(model=\"gpt-4o\", messages=messages)\n        return final_response.choices[0].message.content\n    \n    return msg.content","outputs":[],"execution_count":null,"metadata":{}},{"cell_type":"markdown","source":"-----\n\n### **Phase 3: VS Code Integration (The \"Advanced Version\")**\n\nTo make this professional, we won't just run a script in the terminal. We will build a **VS Code Extension** using the native `ChatParticipant` API (like Copilot) that talks to our Python Agent running as a local server.\n\n#### **1. Project Structure**\n\n```text\nmy-ai-coder/\n├── backend/                 # Python Backend\n│   ├── app.py               # Flask Server\n│   ├── agent.py             # Agent Logic (from Phase 2)\n│   └── requirements.txt\n├── extension/               # VS Code Extension (TypeScript)\n│   ├── src/\n│   │   └── extension.ts     # Main activation file\n│   ├── package.json\n│   └── tsconfig.json\n└── .env                     # Store API Key here\n```\n\n#### **2. The Backend Server (`backend/app.py`)**\n\nWe wrap our agent in a simple Flask server so the VS Code extension can talk to it.","metadata":{}},{"cell_type":"code","source":"from flask import Flask, request, jsonify\nfrom agent import run_agent\n\napp = Flask(__name__)\n\n@app.route('/chat', methods=['POST'])\ndef chat():\n    data = request.json\n    user_query = data.get('query')\n    response = run_agent(user_query)\n    return jsonify({\"response\": response})\n\nif __name__ == '__main__':\n    app.run(port=5000)","outputs":[],"execution_count":null,"metadata":{}},{"cell_type":"markdown","source":"#### **3. The VS Code Extension (`extension/package.json`)**\n\nYou need to tell VS Code that your extension adds a Chat Participant.\n\n```json\n{\n  \"name\": \"my-ai-coder\",\n  \"displayName\": \"My AI Coder\",\n  \"version\": \"0.0.1\",\n  \"engines\": { \"vscode\": \"^1.85.0\" },\n  \"categories\": [\"AI\", \"Chat\"],\n  \"activationEvents\": [],\n  \"main\": \"./out/extension.js\",\n  \"contributes\": {\n    \"chatParticipants\": [\n      {\n        \"id\": \"my-ai.agent\",\n        \"name\": \"agent\",\n        \"fullName\": \"My AI Agent\",\n        \"description\": \"Ask me to analyze code, fix bugs, or run commands!\",\n        \"isSticky\": true\n      }\n    ]\n  }\n}\n```\n\n#### **4. The Activation File (`extension/src/extension.ts`)**\n\nThis is the bridge. It activates, retrieves your API key securely, and forwards chat messages to your running Python server.\n\n```typescript\nimport * as vscode from 'vscode';\n\nexport function activate(context: vscode.ExtensionContext) {\n\n    // 1. Create the Chat Participant\n    const agent = vscode.chat.createChatParticipant('my-ai.agent', async (request, context, stream, token) => {\n        \n        // Show loading indicator\n        stream.progress('Thinking...');\n\n        try {\n            // 2. Send the user's prompt to your local Python Backend\n            const response = await fetch('http://localhost:5000/chat', {\n                method: 'POST',\n                headers: { 'Content-Type': 'application/json' },\n                body: JSON.stringify({ query: request.prompt })\n            });\n\n            const data = await response.json();\n            \n            // 3. Stream the response back to the VS Code Chat UI\n            stream.markdown(data.response);\n            \n        } catch (err) {\n            stream.markdown(\"Error connecting to Agent Backend. Is `app.py` running?\");\n        }\n    });\n\n    agent.iconPath = vscode.Uri.joinPath(context.extensionUri, 'icon.png');\n    context.subscriptions.push(agent);\n\n    // --- SECURE API KEY STORAGE ---\n    // Register a command to save API Key securely\n    let disposable = vscode.commands.registerCommand('my-ai.setApiKey', async () => {\n        const key = await vscode.window.showInputBox({ \n            prompt: \"Enter your OpenAI API Key\", \n            password: true \n        });\n        if (key) {\n            await context.secrets.store(\"OPENAI_API_KEY\", key);\n            vscode.window.showInformationMessage(\"API Key saved securely!\");\n        }\n    });\n    context.subscriptions.push(disposable);\n}\n\nexport function deactivate() {}\n```\n\n-----\n\n### **How to Run It**\n\n1.  **Start the Backend:**\n      * Open a terminal in `my-ai-coder/backend`.\n      * Run: `python app.py` (Ensure you have `.env` with your key or pass it in).\n2.  **Start the Extension:**\n      * Open `my-ai-coder/extension` in VS Code.\n      * Press `F5`. A new \"Extension Development Host\" window will open.\n3.  **Chat:**\n      * In the new window, open the Chat View (Sidebar).\n      * Type `@agent Read my main.py file and tell me what it does.`\n      * Your Python agent will receive the request, use the `read_file` tool, and explain the code back in the UI\\!\n\n### **Alternative: Existing Frameworks**\n\nIf you don't want to write all this code from scratch, you can use these frameworks which plug into VS Code:\n\n  * **Cline (formerly Claude Dev):** An open-source extension that already implements the \"Agent\" loop. You just bring your API key. It can read/write files and run terminal commands out of the box.\n  * **Continue:** A leading open-source autopilot. It's easier to configure than building from scratch but less customizable than writing your own Python agent backend.\n\nFor a true \"do-it-yourself\" learning experience, building the Python + Flask + VS Code Extension stack above is the best path.\n\n[VS Code Extension Chat API Tutorial](https://www.youtube.com/watch?v=X7sTVZ0EbQs)\nThis video is relevant because it provides a visual guide to setting up VS Code extensions and working with the environment, which parallels the setup required for the extension portion of this guide.","metadata":{}}],"metadata":{"colab":{"from_bard":true},"kernelspec":{"display_name":"Python 3","name":"python3"}},"nbformat":4,"nbformat_minor":0}